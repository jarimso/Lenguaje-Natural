{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Básico con Spacy\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion1/1-spacy-basics.ipynb)\n",
    "\n",
    "## Referencias\n",
    "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)\n",
    "* [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "Este notebook contiene ejemplos básico de uso de la librería Spacy para procesamiento de lenguaje natural con técnicas clásicas. Esta herramienta nos servirá para familiarizarnos con los métodos clásicos.\n",
    "\n",
    "## Preparación del entorno\n",
    "Asumiendo que la librería ya se encuentra instalada, dependiendo de la tarea, necesitamos descargar un corpus, por ejemplo en el idioma ingles sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cual debemos luego importar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp39-cp39-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (25.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[K     |████████████████████████████████| 444 kB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.10-cp39-cp39-macosx_11_0_arm64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 28.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 27.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.6-cp39-cp39-macosx_11_0_arm64.whl (848 kB)\n",
      "\u001b[K     |████████████████████████████████| 848 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.1-cp39-cp39-macosx_11_0_arm64.whl (635 kB)\n",
      "\u001b[K     |████████████████████████████████| 635 kB 25.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.13-cp39-cp39-macosx_11_0_arm64.whl (26 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.11-cp39-cp39-macosx_11_0_arm64.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.0.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 49.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 54.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 54.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<1.4.0,>=1.3.0\n",
      "  Downloading blis-1.3.0.tar.gz (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25h\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/4k/lk42gr_94f92nbc9wl3hlffh0000gn/T/pip-build-env-vvt4rxbw/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/4k/lk42gr_94f92nbc9wl3hlffh0000gn/T/pip-build-env-vvt4rxbw/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/4k/lk42gr_94f92nbc9wl3hlffh0000gn/T/pip-build-env-vvt4rxbw/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/4k/lk42gr_94f92nbc9wl3hlffh0000gn/T/pip-build-env-vvt4rxbw/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/4k/lk42gr_94f92nbc9wl3hlffh0000gn/T/pip-build-env-vvt4rxbw/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/4k/lk42gr_94f92nbc9wl3hlffh0000gn/T/pip-build-env-vvt4rxbw/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 19.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rich>=10.11.0\n",
      "  Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 71.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt\n",
      "  Downloading wrapt-1.17.3-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Building wheels for collected packages: blis\n",
      "  Building wheel for blis (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for blis: filename=blis-1.3.0-cp39-cp39-macosx_10_9_universal2.whl size=1495360 sha256=cfa32536999cb66e45818f398ef15c9b63b62da85a236d5520ad2e6efaceb54f\n",
      "  Stored in directory: /Users/j.ricardomunoz/Library/Caches/pip/wheels/2b/75/f4/e540f5b1ef1d2610200886c45092f5be21cfe8309ccc48c577\n",
      "Successfully built blis\n",
      "Installing collected packages: mdurl, typing-inspection, pydantic-core, markdown-it-py, catalogue, annotated-types, wrapt, urllib3, srsly, shellingham, rich, pydantic, murmurhash, marisa-trie, idna, cymem, click, certifi, wasabi, typer, smart-open, requests, preshed, MarkupSafe, language-data, confection, cloudpathlib, blis, weasel, tqdm, thinc, spacy-loggers, spacy-legacy, langcodes, jinja2, spacy\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.8.3 click-8.1.8 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 requests-2.32.4 rich-14.1.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 tqdm-4.67.1 typer-0.16.0 typing-inspection-0.4.1 urllib3-2.5.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Download the model if not already present\n",
    "try:\n",
    "\tnlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "\tfrom spacy.cli import download\n",
    "\tdownload('en_core_web_sm')\n",
    "\tnlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un documento simple\n",
    "Este documento será automáticamente interpretado con spacy para el lenguaje seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde aquí, podemos observar los diferentes elementos del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token               POS                 S-dep               \n",
      "Tesla               PROPN               nsubj\n",
      "is                  AUX                 aux\n",
      "looking             VERB                ROOT\n",
      "at                  ADP                 prep\n",
      "buying              VERB                pcomp\n",
      "U.S.                PROPN               dobj\n",
      "startup             VERB                advcl\n",
      "for                 ADP                 prep\n",
      "$                   SYM                 quantmod\n",
      "6                   NUM                 compound\n",
      "million             NUM                 pobj\n"
     ]
    }
   ],
   "source": [
    "col1 = \"Token\"\n",
    "col2 = \"POS\" # Part of Speech\n",
    "col3 = \"S-dep\" # Syntactic dependency\n",
    "\n",
    "print(f\"{col1:{20}}{col2:{20}}{col3:{20}}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos impreso los tokens (palabras en este caso), la parte del contexto que representan (POS) y la dependencia semantica que dicho token tiene.\n",
    "\n",
    "En NLP clásico hay una taxonomía especializada para cada elemento del lenguaje. Cada elemento fue producto de estudios diversos y variados con el fin de ofrecer un modelado sistemático del lenguaje. Expertos en lenguaje estuvieron involucrados en la creación de esta taxonomía.\n",
    "\n",
    "Ahora, librerías como Spacy facilitan el procesamiento de esta taxonomía."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un pipeline simple de Spacy\n",
    "\n",
    "El núcleo de Spacy es el pipeline que no es más que el procesamiento/transformación que toma el texto original y se lo somete a diversos procesos de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x134c41f40>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x134c41c40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x134ae6350>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x134ccb9c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x134c97140>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x134ae6200>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar aquí, la instanciación por defecto es un pipeline compuesto por diferentes componentes que deberían ser familiares para nosotros:\n",
    "\n",
    "* Token 2 Vec: Convertir los tokens en vectores.\n",
    "* Lemmatizer: Extracción de componentes raíz de las palabras\n",
    "* NER: Named entity recognition para identificar los sujetos de los documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un documento es iterable y los items pueden ser accedidos por índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th token in the document is: Tesla\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "print(f\"The {n}th token in the document is: {doc[n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploremos diferentes elementos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "import pandas as pd\n",
    "\n",
    "def get_doc_elements(doc: Doc):\n",
    "    elements = [\"text\", \"lemma\", \"pos\", \"tag\", \"shape\", \"alpha\", \"stop\"]\n",
    "    rows = [ [token.text, token.lemma_, token.pos_, token.tag_, token.shape_, token.is_alpha, token.is_stop] \n",
    "            for token in  doc]\n",
    "    return pd.DataFrame(rows, columns=elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>shape</th>\n",
       "      <th>alpha</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking</td>\n",
       "      <td>look</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buying</td>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>X.X.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startup</td>\n",
       "      <td>startup</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text    lemma    pos  tag  shape  alpha   stop\n",
       "0     Tesla    Tesla  PROPN  NNP  Xxxxx   True  False\n",
       "1        is       be    AUX  VBZ     xx   True   True\n",
       "2   looking     look   VERB  VBG   xxxx   True  False\n",
       "3        at       at    ADP   IN     xx   True   True\n",
       "4    buying      buy   VERB  VBG   xxxx   True  False\n",
       "5      U.S.     U.S.  PROPN  NNP   X.X.  False  False\n",
       "6   startup  startup   VERB  VBD   xxxx   True  False\n",
       "7       for      for    ADP   IN    xxx   True   True\n",
       "8         $        $    SYM    $      $  False  False\n",
       "9         6        6    NUM   CD      d  False  False\n",
       "10  million  million    NUM   CD   xxxx   True  False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_elements = get_doc_elements(doc)\n",
    "doc_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done:\n",
    "\n",
    "|Tag|Descrición|doc2[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n",
    "|`.lemma_`|The base form of the word|`tesla`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetos Span\n",
    "Un span puede interpretarse como una porción de un documento, es decir, puede empezar desde alún índice hasta otro. Esto facilita el procesamiento por pedazos (chunks) en lugar el documento completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definition of NLP according to Wikipedia \n",
    "doc = nlp(u\"Natural language processing (NLP) is a subfield of computer science, \\\n",
    "information engineering, and artificial intelligence concerned with the \\\n",
    "interactions between computers and human (natural) languages, in particular \\\n",
    "how to program computers to process and analyze large amounts of natural language data.\\\n",
    "Challenges in natural language processing frequently involve speech recognition, natural \\\n",
    "language understanding, and natural language generation.\")\n",
    "\n",
    "quote = doc[10:30]\n",
    "quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos aquí que el slice es por los tokens y no por los caracteres individuales. Esto es muy útil ya que podemos estar seguros de no interrumpir abruptamente los tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con oraciones\n",
    "Podemos iterar sobre oraciones en los documentos, es decir, frases separadas por el punto \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second sentence.\n",
      "And this is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is the first sentence. This is the second sentence. And this is the last sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Cada punto es considerado un token, etnonces en el segundo \"This\" en el anterior documento está en el índice `6`, no en el `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 5: .\n",
      "Token 6: This\n",
      "Is token 6 a sentence start? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token 5: {doc[5]}\")\n",
    "print(f\"Token 6: {doc[6]}\")\n",
    "print(f\"Is token 6 a sentence start? {doc[6].is_sent_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: jinja2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: wrapt in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/j.ricardomunoz/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy) (3.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: command not found: python\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DET det\n",
      "lenguaje NOUN nsubj\n",
      "natural ADJ amod\n",
      "es AUX cop\n",
      "fascinante ADJ ROOT\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# Instala el modelo de español\n",
    "%pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "\n",
    "# Cárgalo en tu código\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Procesa texto en español\n",
    "doc = nlp(\"El lenguaje natural es fascinante.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token               POS                 S-dep               \n",
      "El                  DET                 det\n",
      "lenguaje            NOUN                nsubj\n",
      "natural             ADJ                 amod\n",
      "es                  AUX                 cop\n",
      "fascinante          ADJ                 ROOT\n",
      ".                   PUNCT               punct\n"
     ]
    }
   ],
   "source": [
    "col1 = \"Token\"\n",
    "col2 = \"POS\" # Part of Speech\n",
    "col3 = \"S-dep\" # Syntactic dependency\n",
    "\n",
    "print(f\"{col1:{20}}{col2:{20}}{col3:{20}}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x15117bf40>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x15117bb80>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1511a4ac0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1511753c0>),\n",
       " ('lemmatizer', <spacy.lang.es.lemmatizer.SpanishLemmatizer at 0x1511dd140>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1513440b0>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th token in the document is: El\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "print(f\"The {n}th token in the document is: {doc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "import pandas as pd\n",
    "\n",
    "def get_doc_elements(doc: Doc):\n",
    "    elements = [\"text\", \"lemma\", \"pos\", \"tag\", \"shape\", \"alpha\", \"stop\"]\n",
    "    rows = [ [token.text, token.lemma_, token.pos_, token.tag_, token.shape_, token.is_alpha, token.is_stop] \n",
    "            for token in  doc]\n",
    "    return pd.DataFrame(rows, columns=elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>shape</th>\n",
       "      <th>alpha</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El</td>\n",
       "      <td>el</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "      <td>Xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lenguaje</td>\n",
       "      <td>lenguaje</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natural</td>\n",
       "      <td>natural</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es</td>\n",
       "      <td>ser</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fascinante</td>\n",
       "      <td>fascinante</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text       lemma    pos    tag shape  alpha   stop\n",
       "0          El          el    DET    DET    Xx   True   True\n",
       "1    lenguaje    lenguaje   NOUN   NOUN  xxxx   True  False\n",
       "2     natural     natural    ADJ    ADJ  xxxx   True  False\n",
       "3          es         ser    AUX    AUX    xx   True   True\n",
       "4  fascinante  fascinante    ADJ    ADJ  xxxx   True  False\n",
       "5           .           .  PUNCT  PUNCT     .  False  False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_elements = get_doc_elements(doc)\n",
    "doc_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es el primer paso crucial para que una máquina procese"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definition of NLP according to Wikipedia \n",
    "doc = nlp(u\"En resumen, el notebook te enseña que la tokenización es el primer paso crucial \" \\\n",
    "\"para que una máquina procese texto, y que herramientas como Spacy no solo dividen\\\n",
    "      el texto, sino que también lo analizan para entender su estructura y significado.\")\n",
    "\n",
    "quote = doc[10:20]\n",
    "quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
