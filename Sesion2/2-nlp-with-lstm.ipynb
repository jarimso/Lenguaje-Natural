{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4efb44e",
   "metadata": {},
   "source": [
    "# NLP con Long-Short Term Memory (LSTM)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion2/2-nlp-with-lstm.ipynb)\n",
    "\n",
    "En este notebook implementaremos un clasificador de noticias en espaÃ±ol utilizando la arquitectura de red LSTM. La idea es tener un punto de referencia para comparar cuando observemos la parte de transformers, por lo que utilizaremos el mismo dataset y tarea de ejemplo. UtilizarÃ©mos las utilidades de tokenizaciÃ³n de huggingface transformers para ayudarnos con esta tarea.\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/MarcOrfilaCarreras/spanish-news\n",
    "- [Long Short-Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory#fullTextFileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936855e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34267/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2013edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df40a98",
   "metadata": {},
   "source": [
    "### Cargando el dataset\n",
    "Este es un dataset pequeÃ±o de articulos de noticias en idioma espaÃ±ol con sus respectivas categorÃ­as. El dataset estÃ¡ disponible en el HuggingFace Hub y puede ser fÃ¡cilmente descargado con la librerÃ­a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b91601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 1.14kB [00:00, 1.31MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.8M/44.8M [00:10<00:00, 4.14MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10200/10200 [00:00<00:00, 23599.51 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['language', 'category', 'newspaper', 'hash', 'text'],\n",
       "    num_rows: 10200\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "dataset = load_dataset('MarcOrfilaCarreras/spanish-news', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a53ebd",
   "metadata": {},
   "source": [
    "Observemos uno de sus registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14abea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'es',\n",
       " 'category': 'play',\n",
       " 'newspaper': 'de_lector_a_lector',\n",
       " 'hash': 'b387bc0a5ad68524c8aa5da489555ca41d5a3575',\n",
       " 'text': 'El coraje de ser, de MÃ³nica CavallÃ©, la aventura del autoconocimiento filosÃ³fico.Todos experimentamos momentos de plenitud vinculados a la expresiÃ³n directa y autÃ©ntica de nosotros mismos: momentos de contemplaciÃ³n de la belleza del mundo en que nuestros sentidos se abren como si lo vieran por primera vez, de intimidad y comuniÃ³n con otro ser humano, de fluidez creativa, de expresiÃ³n confiada y libreâ€¦ Estos momentos permiten intuir lo que puede ser una vida en la que no meramente se existe, sino en la que se vive en todo el sentido de esta palabra.Esta vida solo es posible cuando sabemos quiÃ©nes somos, cuando nos conocemos a nosotros mismos de modo experiencial: no cuando nos llenamos de ideas sobre nosotros, sino cuando nos asentamos en nuestro ser real, mÃ¡s allÃ¡ de nuestras defensas, mÃ¡scaras y falsos yoes.El coraje de ser es una invitaciÃ³n a adentrarnos de forma prÃ¡ctica en el camino del autoconocimiento sapiencial y, mÃ¡s ampliamente, en la vida filosÃ³fica. Busca inspirar y acompaÃ±ar en la apasionante aventura de desnudarnos, reconocer nuestra vulnerabilidad, para poder vernos y ser llenados. Solo esta desnudez lÃºcida da paso a una vida creativa y verdadera; una vida que no solo es una bendiciÃ³n para nosotros mismos, sino tambiÃ©n para los demÃ¡s y para el mundo.MÃ³nica CavallÃ© es doctora en FilosofÃ­a por la Universidad Complutense de Madrid y mÃ¡ster en Ciencias de las Religiones. Ha sido profesora de FilosofÃ­a PrÃ¡ctica y durante varios aÃ±os ha coordinado en la Universidad Complutense de Madrid los seminarios de IntroducciÃ³n FilosÃ³fica al Hinduismo y al Budismo.Trabaja como filÃ³sofa, asesora y dirige la Escuela de FilosofÃ­a Sapiencial. Entre sus obras escritas destacan La sabidurÃ­a recobrada, El arte de ser, y\\xa0La sabidurÃ­a de la no-dualidad, publicadas por KairÃ³s.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2676",
   "metadata": {},
   "source": [
    "Para los efectos de esta tarea, nos servirÃ¡n el texto y la categorÃ­a naturalmente.\n",
    "\n",
    "A manera general, observemos que tan largos o cortos tienden a ser los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6642761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto mÃ¡s corto: 501\n",
      "Texto mÃ¡s largo: 204324\n",
      "Longitud promedio: 4218.154509803921\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(row['text']) for row in dataset]\n",
    "print(f\"Texto mÃ¡s corto: {min(text_lengths)}\")\n",
    "print(f\"Texto mÃ¡s largo: {max(text_lengths)}\")\n",
    "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19905224",
   "metadata": {},
   "source": [
    "Estos valores son la cantidad de *caractÃ©res* que tiene las secuencias. Una decisiÃ³n ingenua pero Ãºtil en este momento podrÃ­a ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 2000 tokens. Esto podrÃ­a ser suficiente para capturar una porciÃ³n significativa de los textos.\n",
    "\n",
    "## Definiendo el Tokenizer\n",
    "\n",
    "Ahora, vamos a definir el tokenizer para nuestra tarea. Para ahorrarnos tiempo, vamos a entrenar uno basado en gpt2, pero ajustandolo a nuestro dataset. Para ello, debemos seleccionar una muestra representativa de nuestro dataset, como no es muy grande, casi que podemos usarlo todo. Luego, debemos definir el tamaÃ±o del vocabulario, es decir, cuantos tokens Ãºnicos queremos soportar en nuestro tokenizador. Para que un modelo de lenguaje funcione moderadamente bien para una tarea de clasificaciÃ³n, considerando el tamaÃ±o de nuestro corpus, deberÃ­amos definir unos 50 mil tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39fb654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:06<00:00, 159.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "\n",
    "length = 10000\n",
    "iter_dataset = iter(dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "def batch_iterator(batch_size: int = 10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['text'] for _ in range(batch_size)]\n",
    "\n",
    "spanish_news_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=50000, initial_alphabet=base_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ce90e",
   "metadata": {},
   "source": [
    "Exploremos ahora el tokenizador obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71960c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 50000 tokens\n",
      "Primeros 15 tokens:\n",
      "['<|endoftext|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.']\n",
      "15 tokens de en medio:\n",
      "['pul', 'ece', ' Car', ' op', ' hech', 'ome', ' Ar', ' cen', 'ensa', 'ismo', 'Ã³s', 'ord', 'at', 'â€.', 'lec']\n",
      "Ãšltimos 15 tokens:\n",
      "['yang', 'zidina', 'zambique', '\\x96', ' ud', ' ï¿½', 'deado', ' each', ' eches', ' eternidad', ' eferentes', ' eBook', ' deple', ' dedu', ' deduce']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(spanish_news_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print(f\"Vocabulario: {spanish_news_tokenizer.vocab_size} tokens\")\n",
    "print(\"Primeros 15 tokens:\")\n",
    "print([f\"{spanish_news_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[:15]])\n",
    "print(\"15 tokens de en medio:\")\n",
    "print([f\"{spanish_news_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[1000:1015]])\n",
    "print(\"Ãšltimos 15 tokens:\")\n",
    "print([f\"{spanish_news_tokenizer.convert_tokens_to_string([t])}\" for t, _ in tokens[-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0668c",
   "metadata": {},
   "source": [
    "Vemos que los primeros tokens corresponden a caracteres especiales y puntiaciÃ³n. Luego en el medio tenemos una combinaciÃ³n entre palabras completas y cortadas, el tokenizador se encarga de encontrar las frecuencias mÃ¡s comunes y asi partir las palabras por aquellas partes que tienden a repetirse mas. Esto es muy Ãºtil para trabajar con modelos de lenguaje ya que el modelo se vuelve robusto a diferentes ramificaciones de palabras e incluso a errores de tipografÃ­a. Finalmente, al final, vemos que tenemos mÃ¡s palabras cortadas y palabras muy especiales. Algo importante aquÃ­ es que podamos ver que los tokens tienen sentido con respecto a nuestro corpus.\n",
    "\n",
    "Ahora veamos como convierte el tokenizador una oraciÃ³n muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e68c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [72, 1086, 1039, 1, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_news_tokenizer.pad_token = '[PAD]'\n",
    "spanish_news_tokenizer(\"hola mundo!\", max_length=8, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e6557",
   "metadata": {},
   "source": [
    "Lo que obtenemos de vuelta son los ids de cada token segÃºn el vocabulario. Ahora algo importante que notamos aquÃ­ es el *padding*, durante el entrenamiento, queremos que las secuencias sean de tamaÃ±o fijo, para asi operar comodamente con matrices. Pero ya vimos que no todos los textos tienen la misma longitud. Entonces que hacer? para los que son mÃ¡s largos que una longitud dada simplemente cortamos, pero para los que son mÃ¡s cortos, debemos *rellenar* lo faltante con un *token especial de relleno o padding*. Y es justo lo que definimos allÃ­, cuando la cadena es inferior a 8 **tokens**, entonces debemos hacer padding hasta que se cumplan los 8.\n",
    "\n",
    "Ahora, notemos que \"hola mundo!\" son 2 palabras, 9 letras, 1 espacio y 1 simbolo para un total de 11 caracteres, pero vemos que el resultado son 4 tokens y el padding. Esto es trabajo del tokenizador. Cuando lo entrenamos con nuestro corpus, el tokenizador computÃ³ las frecuencias de palabras y sus partes, tal como vimos arriba, entonces, estos tokens juntos forman la frase original, observemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a812a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'ola', 'Ä mundo', '!', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_news_tokenizer(\"hola mundo!\", max_length=8, truncation=True, padding='max_length').tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18e433",
   "metadata": {},
   "source": [
    "Claramente vemos los 4 tokens como cadenas independientes.\n",
    "\n",
    "### Definiendo el dataset de pytorch\n",
    "Ahora podemos proceder a definir el dataset. Esto deberÃ­a ser muy sencillo dado que nuestro dataset es pequeÃ±o y ya tenemos el tokenizador listo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca70e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpanishNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = '[PAD]'\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        # Definimos estos dos mapas para facilitarnos la tarea\n",
    "        # de traducir de nombres de categorÃ­a a ids de categorÃ­a.\n",
    "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['category'])))\n",
    "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
    "        self.num_classes = len(self.id_2_class_map)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        text, y = self.dataset[index]['text'], self.dataset[index]['category']\n",
    "        y = self.class_2_id_map[y]\n",
    "        data = {k: torch.tensor(v) for k, v in self.tokenizer(text, max_length=self.seq_length, truncation=True, padding='max_length').items()}\n",
    "        data['y'] = torch.tensor(y)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de3493",
   "metadata": {},
   "source": [
    "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaÃ±o mÃ¡ximo de secuencia de 2048 **tokens**. Que segÃºn nuestra intuiciÃ³n arriba, deberÃ­a ser suficiente para la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15200ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512 \n",
    "spanish_news_dataset = SpanishNewsDataset(spanish_news_tokenizer, dataset, seq_length=max_len)\n",
    "assert len(spanish_news_dataset) == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d4b0b",
   "metadata": {},
   "source": [
    "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7e9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4 if not IN_COLAB else 12\n",
    "train_dataset, val_dataset, test_dataset = random_split(spanish_news_dataset, lengths=[0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac44796",
   "metadata": {},
   "source": [
    "## DefiniciÃ³n del modelo LSTM\n",
    "\n",
    "Ahora vamos a configurar un mÃ³dulo pytorch simple para este problema. Vamos ha utilizar los embeddings, que vendrÃ­an siendo los vectores de palabra. Pytorch nos ofrece una capa con la que directamente podemos entrenarlos a partir de los token ids obtenidos. El resto consistirÃ¡ en invocar una capa LSTM seguida de una capa densa para la clasificaciÃ³n.\n",
    "\n",
    "Recordemos que las redes recurrentes como las LSTM por diseÃ±o enlazan todas las dimensiones del vector de entrada, formando asÃ­ la secuencia, la estructura natural que necesitamos representar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d38eb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        return hidden[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f68240",
   "metadata": {},
   "source": [
    "### DefiniciÃ³n del clasificador\n",
    "\n",
    "Finalmente, definimos el modelo en si. Este modelo constarÃ¡ de 3 capas:\n",
    "\n",
    "- La tokenizaciÃ³n, tal como la definimos anteriormente.\n",
    "- El bloque LSTM, que acabamos de decinir.\n",
    "- Una capa densa adicional que servirÃ¡ como clasificador de aquello que nos entregue la capa del transformer.\n",
    "\n",
    "Como este es un LightningModule, aquÃ­ definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d724f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | lstm       | LSTMBlock          | 13.1 M | train\n",
      "1 | classifier | Sequential         | 200 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "13.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.3 M    Total params\n",
      "53.321    Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2040/2040 [02:31<00:00, 13.50it/s, v_num=1, val-loss=2.340, val-acc=0.158, train-loss=2.280, train-acc=0.181]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2040/2040 [02:31<00:00, 13.47it/s, v_num=1, val-loss=2.340, val-acc=0.158, train-loss=2.280, train-acc=0.181]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class SpanishNewsClassifierWithLSTM(LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_classes: int, emb_dim: int, hidden_dim: int = 128):\n",
    "        super(SpanishNewsClassifierWithLSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = LSTMBlock(vocab_size, emb_dim, hidden_dim, num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.lstm(x)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat, y)\n",
    "        self.log('train-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train-acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.val_acc(y_hat, y)\n",
    "        self.log('val-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val-acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        x, y = batch['input_ids'], batch['y']\n",
    "        y_hat = self(x)\n",
    "        self.test_acc(y_hat, y)\n",
    "        self.log('test-acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        x = batch['input_ids']\n",
    "        return self(x)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "model = SpanishNewsClassifierWithLSTM(vocab_size=spanish_news_tokenizer.vocab_size, num_classes=spanish_news_dataset.num_classes, emb_dim=256)\n",
    "\n",
    "tb_logger = TensorBoardLogger('tb_logs', name='LSTMClassifier')\n",
    "callbacks=[EarlyStopping(monitor='train-loss', patience=3, mode='min')]\n",
    "trainer = Trainer(max_epochs=10, devices=1, logger=tb_logger, callbacks=callbacks, precision=\"16-mixed\")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7312d7c",
   "metadata": {},
   "source": [
    "Observemos el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65f9b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e6dc01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cec7e82055e454e5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cec7e82055e454e5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd09d6",
   "metadata": {},
   "source": [
    "Y como es de esperarse, realizaremos la validaciÃ³n contra el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d52f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:02<00:00, 119.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.15980392694473267    </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.15980392694473267   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test-acc': 0.15980392694473267}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d232",
   "metadata": {},
   "source": [
    "### Haciendo predicciones\n",
    "\n",
    "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificaciÃ³n de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff1c989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:01<00:00, 130.29it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "predictions = torch.argmax(predictions, dim=-1)\n",
    "predictions = [spanish_news_dataset.id_2_class_map[pred] for pred in predictions.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bdacb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1709 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_string</th>\n",
       "      <th>categorÃ­a</th>\n",
       "      <th>predicciÃ³n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>TAG Heuer acaba de anunciar una curiosa y pecu...</td>\n",
       "      <td>[27693, 46981, 3747, 259, 11280, 347, 17325, 2...</td>\n",
       "      <td>[TAG, Ä Heuer, Ä acaba, Ä de, Ä anunciar, Ä una, Ä c...</td>\n",
       "      <td>tech</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>DecÃ­a la mitologÃ­a que el Ave FÃ©nix es aquel q...</td>\n",
       "      <td>[36, 4961, 280, 20555, 288, 289, 348, 710, 456...</td>\n",
       "      <td>[D, ecÃƒÅƒa, Ä la, Ä mitologÃƒÅƒa, Ä que, Ä el, Ä A, ve...</td>\n",
       "      <td>sport</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5676</th>\n",
       "      <td>Un aÃ±o mÃ¡s, el concurso que elige a la mejor c...</td>\n",
       "      <td>[2183, 795, 383, 12, 289, 9091, 288, 22358, 26...</td>\n",
       "      <td>[Un, Ä aÃƒÂ±o, Ä mÃƒÂ¡s, ,, Ä el, Ä concurso, Ä que, Ä e...</td>\n",
       "      <td>alimentation</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>El pasado mes de febrero, el inversor George S...</td>\n",
       "      <td>[544, 1145, 946, 259, 1937, 12, 289, 23175, 96...</td>\n",
       "      <td>[El, Ä pasado, Ä mes, Ä de, Ä febrero, ,, Ä el, Ä in...</td>\n",
       "      <td>tech</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>RepartirÃ¡ un dividendo complementario de 0,09 ...</td>\n",
       "      <td>[4716, 493, 44812, 297, 14285, 28343, 259, 165...</td>\n",
       "      <td>[Re, par, tirÃƒÂ¡, Ä un, Ä dividendo, Ä complementa...</td>\n",
       "      <td>economy</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>Volvo sigue siendo noticia en los Ãºltimos dÃ­as...</td>\n",
       "      <td>[42631, 1944, 1397, 5268, 279, 313, 1664, 1253...</td>\n",
       "      <td>[Volvo, Ä sigue, Ä siendo, Ä noticia, Ä en, Ä los, ...</td>\n",
       "      <td>motor</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>Cada cuatro aÃ±os el calendario se ve alterado ...</td>\n",
       "      <td>[10183, 1552, 640, 289, 6394, 309, 885, 34585,...</td>\n",
       "      <td>[Cada, Ä cuatro, Ä aÃƒÂ±os, Ä el, Ä calendario, Ä se,...</td>\n",
       "      <td>economy</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>AnÃ­bal Tortoriello, diputado nacional de Junto...</td>\n",
       "      <td>[2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...</td>\n",
       "      <td>[An, ÃƒÅƒ, bal, Ä Tor, tor, iel, lo, ,, Ä diputado...</td>\n",
       "      <td>politics</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>Las lesiones son una parte fundamental en el d...</td>\n",
       "      <td>[1492, 4361, 558, 347, 769, 3250, 279, 289, 59...</td>\n",
       "      <td>[Las, Ä lesiones, Ä son, Ä una, Ä parte, Ä fundamen...</td>\n",
       "      <td>sport</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Era cuestiÃ³n de tiempo que 'Barbie' siguiera r...</td>\n",
       "      <td>[17838, 3850, 259, 903, 288, 750, 11492, 7, 44...</td>\n",
       "      <td>[Era, Ä cuestiÃƒÂ³n, Ä de, Ä tiempo, Ä que, Ä ', Barb...</td>\n",
       "      <td>play</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>Fuente de la imagen, APEl atÃºn azul constituye...</td>\n",
       "      <td>[3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...</td>\n",
       "      <td>[Fuente, Ä de, Ä la, Ä imagen, ,, Ä AP, El, Ä atÃƒÂºn...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>Esta vez habÃ­a que restregarse los ojos porque...</td>\n",
       "      <td>[3025, 870, 1384, 288, 460, 482, 9569, 313, 57...</td>\n",
       "      <td>[Esta, Ä vez, Ä habÃƒÅƒa, Ä que, Ä res, tre, garse, ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>La fÃ­sica cuÃ¡ntica, ese reino extraÃ±o y maravi...</td>\n",
       "      <td>[606, 4222, 19193, 12, 1082, 12817, 10825, 290...</td>\n",
       "      <td>[La, Ä fÃƒÅƒsica, Ä cuÃƒÂ¡ntica, ,, Ä ese, Ä reino, Ä e...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Hoy dÃ­a no hay nadie que le haga sombra a DJI,...</td>\n",
       "      <td>[7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...</td>\n",
       "      <td>[Hoy, Ä dÃƒÅƒa, Ä no, Ä hay, Ä nadie, Ä que, Ä le, Ä ha...</td>\n",
       "      <td>tech</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>Tecnobit - Grupo OesÃ­a amplÃ­a la familia de ra...</td>\n",
       "      <td>[27489, 17595, 864, 4483, 10768, 16645, 280, 2...</td>\n",
       "      <td>[Tec, nobit, Ä -, Ä Grupo, Ä OesÃƒÅƒa, Ä amplÃƒÅƒa, Ä l...</td>\n",
       "      <td>military</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texto  \\\n",
       "1629  TAG Heuer acaba de anunciar una curiosa y pecu...   \n",
       "4186  DecÃ­a la mitologÃ­a que el Ave FÃ©nix es aquel q...   \n",
       "5676  Un aÃ±o mÃ¡s, el concurso que elige a la mejor c...   \n",
       "1538  El pasado mes de febrero, el inversor George S...   \n",
       "9541  RepartirÃ¡ un dividendo complementario de 0,09 ...   \n",
       "8633  Volvo sigue siendo noticia en los Ãºltimos dÃ­as...   \n",
       "9722  Cada cuatro aÃ±os el calendario se ve alterado ...   \n",
       "6129  AnÃ­bal Tortoriello, diputado nacional de Junto...   \n",
       "3576  Las lesiones son una parte fundamental en el d...   \n",
       "338   Era cuestiÃ³n de tiempo que 'Barbie' siguiera r...   \n",
       "2814  Fuente de la imagen, APEl atÃºn azul constituye...   \n",
       "3686  Esta vez habÃ­a que restregarse los ojos porque...   \n",
       "2952  La fÃ­sica cuÃ¡ntica, ese reino extraÃ±o y maravi...   \n",
       "1261  Hoy dÃ­a no hay nadie que le haga sombra a DJI,...   \n",
       "7204  Tecnobit - Grupo OesÃ­a amplÃ­a la familia de ra...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "1629  [27693, 46981, 3747, 259, 11280, 347, 17325, 2...   \n",
       "4186  [36, 4961, 280, 20555, 288, 289, 348, 710, 456...   \n",
       "5676  [2183, 795, 383, 12, 289, 9091, 288, 22358, 26...   \n",
       "1538  [544, 1145, 946, 259, 1937, 12, 289, 23175, 96...   \n",
       "9541  [4716, 493, 44812, 297, 14285, 28343, 259, 165...   \n",
       "8633  [42631, 1944, 1397, 5268, 279, 313, 1664, 1253...   \n",
       "9722  [10183, 1552, 640, 289, 6394, 309, 885, 34585,...   \n",
       "6129  [2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...   \n",
       "3576  [1492, 4361, 558, 347, 769, 3250, 279, 289, 59...   \n",
       "338   [17838, 3850, 259, 903, 288, 750, 11492, 7, 44...   \n",
       "2814  [3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...   \n",
       "3686  [3025, 870, 1384, 288, 460, 482, 9569, 313, 57...   \n",
       "2952  [606, 4222, 19193, 12, 1082, 12817, 10825, 290...   \n",
       "1261  [7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...   \n",
       "7204  [27489, 17595, 864, 4483, 10768, 16645, 280, 2...   \n",
       "\n",
       "                                          tokens_string     categorÃ­a  \\\n",
       "1629  [TAG, Ä Heuer, Ä acaba, Ä de, Ä anunciar, Ä una, Ä c...          tech   \n",
       "4186  [D, ecÃƒÅƒa, Ä la, Ä mitologÃƒÅƒa, Ä que, Ä el, Ä A, ve...         sport   \n",
       "5676  [Un, Ä aÃƒÂ±o, Ä mÃƒÂ¡s, ,, Ä el, Ä concurso, Ä que, Ä e...  alimentation   \n",
       "1538  [El, Ä pasado, Ä mes, Ä de, Ä febrero, ,, Ä el, Ä in...          tech   \n",
       "9541  [Re, par, tirÃƒÂ¡, Ä un, Ä dividendo, Ä complementa...       economy   \n",
       "8633  [Volvo, Ä sigue, Ä siendo, Ä noticia, Ä en, Ä los, ...         motor   \n",
       "9722  [Cada, Ä cuatro, Ä aÃƒÂ±os, Ä el, Ä calendario, Ä se,...       economy   \n",
       "6129  [An, ÃƒÅƒ, bal, Ä Tor, tor, iel, lo, ,, Ä diputado...      politics   \n",
       "3576  [Las, Ä lesiones, Ä son, Ä una, Ä parte, Ä fundamen...         sport   \n",
       "338   [Era, Ä cuestiÃƒÂ³n, Ä de, Ä tiempo, Ä que, Ä ', Barb...          play   \n",
       "2814  [Fuente, Ä de, Ä la, Ä imagen, ,, Ä AP, El, Ä atÃƒÂºn...     astronomy   \n",
       "3686  [Esta, Ä vez, Ä habÃƒÅƒa, Ä que, Ä res, tre, garse, ...         sport   \n",
       "2952  [La, Ä fÃƒÅƒsica, Ä cuÃƒÂ¡ntica, ,, Ä ese, Ä reino, Ä e...     astronomy   \n",
       "1261  [Hoy, Ä dÃƒÅƒa, Ä no, Ä hay, Ä nadie, Ä que, Ä le, Ä ha...          tech   \n",
       "7204  [Tec, nobit, Ä -, Ä Grupo, Ä OesÃƒÅƒa, Ä amplÃƒÅƒa, Ä l...      military   \n",
       "\n",
       "     predicciÃ³n  \n",
       "1629   military  \n",
       "4186   military  \n",
       "5676       play  \n",
       "1538   medicine  \n",
       "9541   medicine  \n",
       "8633       play  \n",
       "9722   politics  \n",
       "6129       play  \n",
       "3576   medicine  \n",
       "338        play  \n",
       "2814       play  \n",
       "3686       play  \n",
       "2952       play  \n",
       "1261      motor  \n",
       "7204   military  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_indices = test_dataset.indices\n",
    "df = pd.DataFrame(data={\n",
    "    \"texto\": dataset[test_indices]['text'],\n",
    "    \"tokens\": [spanish_news_tokenizer(v)['input_ids'] for v in dataset[test_indices]['text']],\n",
    "    \"categorÃ­a\": dataset[test_indices]['category'],\n",
    "    'predicciÃ³n': predictions\n",
    "}, index=test_indices)\n",
    "\n",
    "df['tokens_string'] = df.tokens.apply(lambda t: spanish_news_tokenizer.convert_ids_to_tokens(t))\n",
    "df = df[[\"texto\", \"tokens\", \"tokens_string\", \"categorÃ­a\", \"predicciÃ³n\"]]\n",
    "df.style.set_table_styles(\n",
    "    [\n",
    "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
    "    ]\n",
    ")\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c5085d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_string</th>\n",
       "      <th>categorÃ­a</th>\n",
       "      <th>predicciÃ³n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>TAG Heuer acaba de anunciar una curiosa y pecu...</td>\n",
       "      <td>[27693, 46981, 3747, 259, 11280, 347, 17325, 2...</td>\n",
       "      <td>[TAG, Ä Heuer, Ä acaba, Ä de, Ä anunciar, Ä una, Ä c...</td>\n",
       "      <td>tech</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>DecÃ­a la mitologÃ­a que el Ave FÃ©nix es aquel q...</td>\n",
       "      <td>[36, 4961, 280, 20555, 288, 289, 348, 710, 456...</td>\n",
       "      <td>[D, ecÃƒÅƒa, Ä la, Ä mitologÃƒÅƒa, Ä que, Ä el, Ä A, ve...</td>\n",
       "      <td>sport</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5676</th>\n",
       "      <td>Un aÃ±o mÃ¡s, el concurso que elige a la mejor c...</td>\n",
       "      <td>[2183, 795, 383, 12, 289, 9091, 288, 22358, 26...</td>\n",
       "      <td>[Un, Ä aÃƒÂ±o, Ä mÃƒÂ¡s, ,, Ä el, Ä concurso, Ä que, Ä e...</td>\n",
       "      <td>alimentation</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>El pasado mes de febrero, el inversor George S...</td>\n",
       "      <td>[544, 1145, 946, 259, 1937, 12, 289, 23175, 96...</td>\n",
       "      <td>[El, Ä pasado, Ä mes, Ä de, Ä febrero, ,, Ä el, Ä in...</td>\n",
       "      <td>tech</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>RepartirÃ¡ un dividendo complementario de 0,09 ...</td>\n",
       "      <td>[4716, 493, 44812, 297, 14285, 28343, 259, 165...</td>\n",
       "      <td>[Re, par, tirÃƒÂ¡, Ä un, Ä dividendo, Ä complementa...</td>\n",
       "      <td>economy</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>Volvo sigue siendo noticia en los Ãºltimos dÃ­as...</td>\n",
       "      <td>[42631, 1944, 1397, 5268, 279, 313, 1664, 1253...</td>\n",
       "      <td>[Volvo, Ä sigue, Ä siendo, Ä noticia, Ä en, Ä los, ...</td>\n",
       "      <td>motor</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>Cada cuatro aÃ±os el calendario se ve alterado ...</td>\n",
       "      <td>[10183, 1552, 640, 289, 6394, 309, 885, 34585,...</td>\n",
       "      <td>[Cada, Ä cuatro, Ä aÃƒÂ±os, Ä el, Ä calendario, Ä se,...</td>\n",
       "      <td>economy</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>AnÃ­bal Tortoriello, diputado nacional de Junto...</td>\n",
       "      <td>[2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...</td>\n",
       "      <td>[An, ÃƒÅƒ, bal, Ä Tor, tor, iel, lo, ,, Ä diputado...</td>\n",
       "      <td>politics</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>Las lesiones son una parte fundamental en el d...</td>\n",
       "      <td>[1492, 4361, 558, 347, 769, 3250, 279, 289, 59...</td>\n",
       "      <td>[Las, Ä lesiones, Ä son, Ä una, Ä parte, Ä fundamen...</td>\n",
       "      <td>sport</td>\n",
       "      <td>medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>Fuente de la imagen, APEl atÃºn azul constituye...</td>\n",
       "      <td>[3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...</td>\n",
       "      <td>[Fuente, Ä de, Ä la, Ä imagen, ,, Ä AP, El, Ä atÃƒÂºn...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>Esta vez habÃ­a que restregarse los ojos porque...</td>\n",
       "      <td>[3025, 870, 1384, 288, 460, 482, 9569, 313, 57...</td>\n",
       "      <td>[Esta, Ä vez, Ä habÃƒÅƒa, Ä que, Ä res, tre, garse, ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>La fÃ­sica cuÃ¡ntica, ese reino extraÃ±o y maravi...</td>\n",
       "      <td>[606, 4222, 19193, 12, 1082, 12817, 10825, 290...</td>\n",
       "      <td>[La, Ä fÃƒÅƒsica, Ä cuÃƒÂ¡ntica, ,, Ä ese, Ä reino, Ä e...</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Hoy dÃ­a no hay nadie que le haga sombra a DJI,...</td>\n",
       "      <td>[7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...</td>\n",
       "      <td>[Hoy, Ä dÃƒÅƒa, Ä no, Ä hay, Ä nadie, Ä que, Ä le, Ä ha...</td>\n",
       "      <td>tech</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>No hay muchos obispos que puedan dirigirse a m...</td>\n",
       "      <td>[1454, 694, 1400, 7004, 288, 3542, 31653, 264,...</td>\n",
       "      <td>[No, Ä hay, Ä muchos, Ä obispos, Ä que, Ä puedan, Ä ...</td>\n",
       "      <td>religion</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8940</th>\n",
       "      <td>El pasado lunes 5 de febrero saliÃ³ a la luz qu...</td>\n",
       "      <td>[544, 1145, 3908, 705, 259, 1937, 9098, 264, 2...</td>\n",
       "      <td>[El, Ä pasado, Ä lunes, Ä 5, Ä de, Ä febrero, Ä sali...</td>\n",
       "      <td>motor</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texto  \\\n",
       "1629  TAG Heuer acaba de anunciar una curiosa y pecu...   \n",
       "4186  DecÃ­a la mitologÃ­a que el Ave FÃ©nix es aquel q...   \n",
       "5676  Un aÃ±o mÃ¡s, el concurso que elige a la mejor c...   \n",
       "1538  El pasado mes de febrero, el inversor George S...   \n",
       "9541  RepartirÃ¡ un dividendo complementario de 0,09 ...   \n",
       "8633  Volvo sigue siendo noticia en los Ãºltimos dÃ­as...   \n",
       "9722  Cada cuatro aÃ±os el calendario se ve alterado ...   \n",
       "6129  AnÃ­bal Tortoriello, diputado nacional de Junto...   \n",
       "3576  Las lesiones son una parte fundamental en el d...   \n",
       "2814  Fuente de la imagen, APEl atÃºn azul constituye...   \n",
       "3686  Esta vez habÃ­a que restregarse los ojos porque...   \n",
       "2952  La fÃ­sica cuÃ¡ntica, ese reino extraÃ±o y maravi...   \n",
       "1261  Hoy dÃ­a no hay nadie que le haga sombra a DJI,...   \n",
       "8260  No hay muchos obispos que puedan dirigirse a m...   \n",
       "8940  El pasado lunes 5 de febrero saliÃ³ a la luz qu...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "1629  [27693, 46981, 3747, 259, 11280, 347, 17325, 2...   \n",
       "4186  [36, 4961, 280, 20555, 288, 289, 348, 710, 456...   \n",
       "5676  [2183, 795, 383, 12, 289, 9091, 288, 22358, 26...   \n",
       "1538  [544, 1145, 946, 259, 1937, 12, 289, 23175, 96...   \n",
       "9541  [4716, 493, 44812, 297, 14285, 28343, 259, 165...   \n",
       "8633  [42631, 1944, 1397, 5268, 279, 313, 1664, 1253...   \n",
       "9722  [10183, 1552, 640, 289, 6394, 309, 885, 34585,...   \n",
       "6129  [2541, 295, 5937, 3868, 403, 3971, 378, 12, 84...   \n",
       "3576  [1492, 4361, 558, 347, 769, 3250, 279, 289, 59...   \n",
       "2814  [3942, 259, 280, 2026, 12, 8978, 544, 13395, 3...   \n",
       "3686  [3025, 870, 1384, 288, 460, 482, 9569, 313, 57...   \n",
       "2952  [606, 4222, 19193, 12, 1082, 12817, 10825, 290...   \n",
       "1261  [7777, 1027, 365, 694, 3428, 288, 540, 6108, 1...   \n",
       "8260  [1454, 694, 1400, 7004, 288, 3542, 31653, 264,...   \n",
       "8940  [544, 1145, 3908, 705, 259, 1937, 9098, 264, 2...   \n",
       "\n",
       "                                          tokens_string     categorÃ­a  \\\n",
       "1629  [TAG, Ä Heuer, Ä acaba, Ä de, Ä anunciar, Ä una, Ä c...          tech   \n",
       "4186  [D, ecÃƒÅƒa, Ä la, Ä mitologÃƒÅƒa, Ä que, Ä el, Ä A, ve...         sport   \n",
       "5676  [Un, Ä aÃƒÂ±o, Ä mÃƒÂ¡s, ,, Ä el, Ä concurso, Ä que, Ä e...  alimentation   \n",
       "1538  [El, Ä pasado, Ä mes, Ä de, Ä febrero, ,, Ä el, Ä in...          tech   \n",
       "9541  [Re, par, tirÃƒÂ¡, Ä un, Ä dividendo, Ä complementa...       economy   \n",
       "8633  [Volvo, Ä sigue, Ä siendo, Ä noticia, Ä en, Ä los, ...         motor   \n",
       "9722  [Cada, Ä cuatro, Ä aÃƒÂ±os, Ä el, Ä calendario, Ä se,...       economy   \n",
       "6129  [An, ÃƒÅƒ, bal, Ä Tor, tor, iel, lo, ,, Ä diputado...      politics   \n",
       "3576  [Las, Ä lesiones, Ä son, Ä una, Ä parte, Ä fundamen...         sport   \n",
       "2814  [Fuente, Ä de, Ä la, Ä imagen, ,, Ä AP, El, Ä atÃƒÂºn...     astronomy   \n",
       "3686  [Esta, Ä vez, Ä habÃƒÅƒa, Ä que, Ä res, tre, garse, ...         sport   \n",
       "2952  [La, Ä fÃƒÅƒsica, Ä cuÃƒÂ¡ntica, ,, Ä ese, Ä reino, Ä e...     astronomy   \n",
       "1261  [Hoy, Ä dÃƒÅƒa, Ä no, Ä hay, Ä nadie, Ä que, Ä le, Ä ha...          tech   \n",
       "8260  [No, Ä hay, Ä muchos, Ä obispos, Ä que, Ä puedan, Ä ...      religion   \n",
       "8940  [El, Ä pasado, Ä lunes, Ä 5, Ä de, Ä febrero, Ä sali...         motor   \n",
       "\n",
       "     predicciÃ³n  \n",
       "1629   military  \n",
       "4186   military  \n",
       "5676       play  \n",
       "1538   medicine  \n",
       "9541   medicine  \n",
       "8633       play  \n",
       "9722   politics  \n",
       "6129       play  \n",
       "3576   medicine  \n",
       "2814       play  \n",
       "3686       play  \n",
       "2952       play  \n",
       "1261      motor  \n",
       "8260   politics  \n",
       "8940       play  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = df[df['categorÃ­a'] != df['predicciÃ³n']]\n",
    "errors.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88103e5e",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- En este caso tenemos una tarea de clasificaciÃ³n de texto de mÃºltiples clases.\n",
    "- Estamos usando un bloque LSTM como featurizer, es decir lo usamos para extraer features de las secuencias de entrada con las cuales harÃ©mos predicciones luego.\n",
    "- NÃ³tese que de las capas LSTM, solo nos interesa la Ãºltima, ya que esta recupera todas las operaciones enalazadas anteriores.\n",
    "- Observamos que el modelo toma su tiempo en entrenar, esto es natural debido al diseÃ±o de las LSTM, donde por cada paso de tiempo se debe computar un gradiente, por lo que el computo es mucho mayor.\n",
    "- Los resultados de clasificaciÃ³n no son malos, pero tampoco son excelentes. Podemos hacerlo mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73526f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
